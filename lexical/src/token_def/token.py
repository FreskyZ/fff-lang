# fff-lang
# token base type generator

from functools import reduce

expecting_seperator = False
seperators = []
keywords = []
for line in map(str.strip, open('token.grammar', 'r').readlines()):
    if 'seperator:' in line:
        expecting_seperator = True
        continue
    elif 'keyword:' in line:
        expecting_seperator = False
        continue
    elif line.startswith('//') or len(line) == 0:
        continue # ignore
    value, rest = map(lambda x: x.strip(' \''), line.split(' => '))
    name, category, _ = map(str.strip, rest.split(','))
    if expecting_seperator:
        categories = list(map(str.strip, category.split('|')))
        seperators.append((value, name, categories))
    else:
        keywords.append((value, name, category))

seperators.sort(key = lambda x: x[0])

def format_all():
    max_value_length = 0
    max_name_length = 0
    for keyword in keywords:
        max_value_length = max(max_value_length, len(keyword[0]))
        max_name_length = max(max_name_length, len(keyword[1]))
    for seperator in seperators:
        max_value_length = max(max_value_length, len(seperator[0]))
        max_name_length = max(max_name_length, len(seperator[1]))

    retval = 'seperator:\n'
    for seperator in seperators:
        retval += "{space1}'{value}' => {space2}{name}, {cat},\n".format(
                    value = seperator[0],
                    space1 = ' ' * (max_value_length - len(seperator[0])),
                    space2 = ' ' * (max_name_length - len(seperator[1])),
                    name = seperator[1],
                    cat = ' | '.join(seperator[2]))
    retval += "\nkeyword:\n"
    for keyword in keywords:
        retval += "{space1}'{value}' => {space2}{name}, {cat},\n".format(
                    value = keyword[0],
                    space1 = ' ' * (max_value_length - len(keyword[0])),
                    space2 = ' ' * (max_name_length - len(keyword[1])),
                    name = keyword[1],
                    cat = keyword[2])
    return retval

def about_hasher_something():
    # values: list[T], hasher: f(T): int
    def generate_hasher(values, hasher):
        half_hash_values = list(map(hasher, values))
        def get_bucket_size(moder):
            hash_values = [half_hash_value % moder for half_hash_value in half_hash_values]
            max_collision_count = max(len(list(filter(lambda x: x == hash_value, hash_values))) for hash_value in set(hash_values))
            return moder, max_collision_count, moder * max_collision_count
        return next(iter(sorted(map(get_bucket_size, range(len(values), 1000)), key = lambda x: x[2])), None)

    moder, bucket_number, memory_use = generate_hasher(
        list(map(lambda x: list(map(int, x[0].encode('ascii'))), seperators)),
        lambda bs: bs[0] if len(bs) == 1 else (bs[0] * 256 + bs[1] if len(bs) == 2 else (bs[0] * 65536 + bs[1] * 256 + bs[2])))
    print(f'seperator::parse3 best result: moder = {moder}, bucket_number = {bucket_number}, memory_use = {memory_use}')

    moder, bucket_number, memory_use = generate_hasher(
        list(map(lambda x: x[0], filter(lambda x: len(x[0]) == 2, seperators))),
        lambda x: ord(x[0]) * 256 + ord(x[1]))
    print(f'seperator::parse2 best result: moder = {moder}, bucket_number = {bucket_number}, memory_use = {memory_use}')

    moder, bucket_number, memory_use = generate_hasher(
        list(map(lambda x: ord(x[0][0]), filter(lambda x: len(x[0]) == 1, seperators))), 
        lambda x: x)
    print(f'seperator::parse1 best result: moder = {moder}, bucket_number = {bucket_number}, memory_use = {memory_use}')

    from functools import reduce
    moder, bucket_number, memory_use = generate_hasher(
        list(map(lambda x: x[0], keywords)),
        lambda value: reduce(lambda x, y: x * (ord(y) - 42) % 1800000000000001, value, 1)) # the 2 numbers are found by trials
        # lambda value: reduce(lambda x, y: x * ord(y), value, 1))
    print(f'keyword::parse best result, moder = {moder}, bucket_number = {bucket_number}, memory_use = {memory_use}')

def generate_seperator():
    def extend_list(l, item):
        l.extend(item)
        return l
    src = ''
    src += '///! fff-lang\n'
    src += '///!\n'
    src += '///! lexical/seperator\n'
    src += '///! Attention: contens are auto generated by token.py, do not modify this file\n\n'

    src += '#[allow(non_snake_case)\n'
    src += '#[allow(non_upper_case_globals)\n'
    src += 'pub mod SeperatorCategory {\n    '
    categories = set(reduce(extend_list, map(lambda s: s[2], seperators), []))
    assert len(categories) < 16
    max_categories_len = len(max(categories, key = len))
    src += '\n    '.join('pub const {space}{name}: u16 = {value};'.format(
            space = ' ' * (max_categories_len - len(category[1])),
            name = category[1],
            value = '0x%04x' % (2 ** category[0])
    ) for category in enumerate(categories))
    src += '\n}\n\n'

    src += '#[derive(Eq, PartialEq, Copy, Clone)]\n'
    src += 'pub enum Seperator {\n    '
    src += '\n    '.join('{},'.format(name) for name in map(lambda x: x[1], seperators))
    src += '\n}\n\n'

    # parse1: hasher: lambda x: x, moder/bucket_size: 37, bucket_number: 1
    # TODO: this may not pass test
    single_char_to_name = [255] * 37
    src += 'const SINGLE_CHAR_TO_NAME: &[u8] = &['
    for index, seperator in filter(lambda x: len(x[1][0]) == 1, enumerate(seperators)):
        single_char_to_name[ord(seperator[0][0]) % 37] = index
    for index, item in enumerate(single_char_to_name):
        if index % 16 == 0:
            src += '\n    '
        src += str(item) + ', '
    src += '\n];\n'

    # TODO: parse3
    # first hash(ch1, ch2, ch3) to check for existence
    # assume this require bucket_size, bucket_number = 3
    # then generate ... it's very complex and do it later

    src += 'impl Seperator {\n\n'
    src += '    fn parse1(ch: char) -> Option<Seperator> {\n'
    src += '        ::std::mem::transmute::<u8, Seperator>(SINGLE_CHAR_TO_NAME[ch as u32 % 37])\n'
    src += '    }\n'
    src += '}\n'

    src += 'const SEP_NAMES: &[&str]: &['
    src += ', '.join(map(
        lambda x: '{opt_space}"{value}"'.format(value = x[1][0], opt_space = '\n    ' if x[0] % 12 == 0 else ''), enumerate(seperators)))
    src += '\n];\n'
    src += 'impl ::std::fmt::Debug for Seperator {\n'
    src += '    fn fmt(&self, f: &mut ::std::fmt::Formatter) -> ::std::fmt::Result {\n'
    src += '        unsafe { write!(f, "{{}}",\n'
    src += '            ::std::mem::transmute::<&[u8], &str>(\n'
    src += '                SEP_NAMES[::std::mem::transmute_copy::<Seperator, u8>(self) as usize]\n'
    src += '            )\n'
    src += '        ) }\n'
    src += '    }\n'
    src += '}\n'

    # print(len(set(reduce(extend_list, map(lambda x: list(x[0]), seperators), []))))

    return src

# print(format_all())
about_hasher_something()
#print(generate_seperator())